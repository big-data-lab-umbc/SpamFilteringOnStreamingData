{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from kafka import KafkaConsumer, KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SPARK session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('kafka').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subscribe to \"kafka_stream\" Topic and create \"df1\" dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"kafka_stream\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if data is streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure SPARK partitions to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.writeStream.format(\"console\").outputMode(\"append\").start()\n",
    "#df.createOrReplaceTempView(\"df1\");\n",
    "#records = spark.sql (\"SELECT count(*) from df\") \n",
    "#print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_kafka = df1.select(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgQuery = msg_kafka.writeStream.queryName(\"msg_kafka\")\\\n",
    ".format(\"memory\").outputMode(\"append\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in range(2):\n",
    " #   spark.sql(\"SELECT * FROM msg\").show()\n",
    " #  sleep(1)\n",
    "\n",
    "df_msg = spark.sql(\"SELECT * FROM msg_kafka\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|As a valued netwo...|\n",
      "|As usual u can ca...|\n",
      "|Wat time Ì_ wan t...|\n",
      "|No plans yet. Wha...|\n",
      "|Anything lar then...|\n",
      "|Even u dont get i...|\n",
      "|Was actually abou...|\n",
      "|So how many days ...|\n",
      "|**FREE MESSAGE**T...|\n",
      "|\"I know dat feeli...|\n",
      "|Its a big differe...|\n",
      "|I don know accoun...|\n",
      "|\"Doing nothing, t...|\n",
      "|Pls i wont belive...|\n",
      "|On the way to off...|\n",
      "|Sunshine Quiz Wkl...|\n",
      "|\"Wat time liao, w...|\n",
      "|\"It's fine, imma ...|\n",
      "|I taught that Ran...|\n",
      "|Oh Howda gud gud....|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_msg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tkn = Tokenizer().setInputCol(\"value\").setOutputCol(\"TextTokenized\").transform(df_msg.select(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "engStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = StopWordsRemover().setStopWords(engStopWords).setInputCol(\"TextTokenized\").setOutputCol(\"TextFiltered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.transform(df_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CountVectorizer to get numerical representation of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\\\n",
    ".setInputCol(\"TextFiltered\")\\\n",
    ".setOutputCol(\"TextCV\")\\\n",
    ".setVocabSize(500)\\\n",
    ".setMinTF(1)\\\n",
    ".setMinDF(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = cv.fit(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = df_cv.transform(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|               value|       TextTokenized|        TextFiltered|              TextCV|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|As a valued netwo...|[as, a, valued, n...|[valued, network,...|(500,[1,45,80,140...|\n",
      "|As usual u can ca...|[as, usual, u, ca...|[usual, u, call, ...|(500,[0,1],[1.0,1...|\n",
      "|Wat time Ì_ wan t...|[wat, time, ì_, w...|[wat, time, ì_, w...|(500,[27,61,87,88...|\n",
      "|No plans yet. Wha...|[no, plans, yet.,...| [plans, yet., ?,,,]|   (500,[295],[1.0])|\n",
      "|Anything lar then...|[anything, lar, t...|[anything, lar, ì...|(500,[8,16,30,61,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         ReceivedMsg|            features|\n",
      "+--------------------+--------------------+\n",
      "|As a valued netwo...|(500,[1,45,80,140...|\n",
      "|As usual u can ca...|(500,[0,1],[1.0,1...|\n",
      "|Wat time Ì_ wan t...|(500,[27,61,87,88...|\n",
      "|No plans yet. Wha...|   (500,[295],[1.0])|\n",
      "|Anything lar then...|(500,[8,16,30,61,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = df_cv.selectExpr(\"value as ReceivedMsg\", \"TextCV as features\" )\n",
    "df_input.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes,NaiveBayesModel\n",
    "#model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel=NaiveBayesModel.load(\"./nbModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting - Function to check if the streaming msg is spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def check_if_spam(trainedModel, df_input):\n",
    " #   is_spam = True\n",
    " #   return is_spam\n",
    "\n",
    "output = trainedModel.transform(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|         ReceivedMsg|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|As a valued netwo...|(500,[1,45,80,140...|[-63.515961231592...|[0.02900628350445...|       1.0|\n",
      "|As usual u can ca...|(500,[0,1],[1.0,1...|[-7.8804239280554...|[0.74397915848769...|       0.0|\n",
      "|Wat time Ì_ wan t...|(500,[27,61,87,88...|[-32.454823651635...|[0.98429700525633...|       0.0|\n",
      "|No plans yet. Wha...|   (500,[295],[1.0])|[-8.2946515715809...|[0.34345453136308...|       1.0|\n",
      "|Anything lar then...|(500,[8,16,30,61,...|[-26.095895283588...|[0.99955060122965...|       0.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.withColumn(\"Result\", when (col (\"prediction\") == 1, \"spam\").otherwise(\"ham\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+------+\n",
      "|         ReceivedMsg|            features|       rawPrediction|         probability|prediction|Result|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+------+\n",
      "|As a valued netwo...|(500,[1,45,80,140...|[-63.515961231592...|[0.02900628350445...|       1.0|  spam|\n",
      "|As usual u can ca...|(500,[0,1],[1.0,1...|[-7.8804239280554...|[0.74397915848769...|       0.0|   ham|\n",
      "|Wat time Ì_ wan t...|(500,[27,61,87,88...|[-32.454823651635...|[0.98429700525633...|       0.0|   ham|\n",
      "|No plans yet. Wha...|   (500,[295],[1.0])|[-8.2946515715809...|[0.34345453136308...|       1.0|  spam|\n",
      "|Anything lar then...|(500,[8,16,30,61,...|[-26.095895283588...|[0.99955060122965...|       0.0|   ham|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResult = output.selectExpr(\"ReceivedMsg\", \"Result\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|         ReceivedMsg|Result|\n",
      "+--------------------+------+\n",
      "|As a valued netwo...|  spam|\n",
      "|As usual u can ca...|   ham|\n",
      "|Wat time Ì_ wan t...|   ham|\n",
      "|No plans yet. Wha...|  spam|\n",
      "|Anything lar then...|   ham|\n",
      "|Even u dont get i...|   ham|\n",
      "|Was actually abou...|   ham|\n",
      "|So how many days ...|   ham|\n",
      "|**FREE MESSAGE**T...|   ham|\n",
      "|\"I know dat feeli...|   ham|\n",
      "|Its a big differe...|   ham|\n",
      "|I don know accoun...|   ham|\n",
      "|\"Doing nothing, t...|   ham|\n",
      "|Pls i wont belive...|   ham|\n",
      "|On the way to off...|   ham|\n",
      "|Sunshine Quiz Wkl...|   ham|\n",
      "|\"Wat time liao, w...|   ham|\n",
      "|\"It's fine, imma ...|  spam|\n",
      "|I taught that Ran...|   ham|\n",
      "|Oh Howda gud gud....|   ham|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalResult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResult.write.csv('filterResult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
